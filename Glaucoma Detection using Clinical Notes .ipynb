{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzfsVoYqvMTw"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow scikit-learn pandas matplotlib imbalanced-learn -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/datamining/clinical_notes.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\n', ' ', text)  # Remove newline characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Clean the notes\n",
        "data['note'] = data['note'].apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "MAX_WORDS = 5000\n",
        "MAX_SEQ_LENGTH = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"\")\n",
        "tokenizer.fit_on_texts(data['note'])\n",
        "sequences = tokenizer.texts_to_sequences(data['note'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "label_map = {'yes': 1, 'no': 0}\n",
        "data['glaucoma'] = data['glaucoma'].map(label_map)\n",
        "\n",
        "# Split data based on 'use' column\n",
        "train_indices = data['use'] == 'training'\n",
        "validation_indices = data['use'] == 'validation'\n",
        "test_indices = data['use'] == 'test'\n",
        "\n",
        "X_train, y_train = padded_sequences[train_indices], data['glaucoma'].values[train_indices]\n",
        "X_val, y_val = padded_sequences[validation_indices], data['glaucoma'].values[validation_indices]\n",
        "X_test, y_test = padded_sequences[test_indices], data['glaucoma'].values[test_indices]\n",
        "\n",
        "race_test = data['race'].values[test_indices]  # For racial group evaluation\n",
        "\n",
        "# Handle class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Fairness-Aware Training\n",
        "# The fairness-aware model should try to ensure equal performance across groups\n",
        "# We will use adversarial debiasing or other fairness techniques here if necessary.\n",
        "# In practice, this requires additional setup such as adversarial networks or fairness constraints.\n",
        "\n",
        "# Define Models\n",
        "def build_lstm_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(MAX_WORDS, 128, input_length=MAX_SEQ_LENGTH),\n",
        "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(MAX_WORDS, 128, input_length=MAX_SEQ_LENGTH),\n",
        "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_transformer_model():\n",
        "    input_layer = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,))\n",
        "    embedding = tf.keras.layers.Embedding(MAX_WORDS, 128)(input_layer)\n",
        "    transformer_block = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=4, key_dim=128, dropout=0.1\n",
        "    )(embedding, embedding)\n",
        "    flatten = tf.keras.layers.GlobalAveragePooling1D()(transformer_block)\n",
        "    dense = tf.keras.layers.Dense(64, activation='relu')(flatten)\n",
        "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Tuning using GridSearch (for better model accuracy)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'learning_rate': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Early Stopping and Learning Rate Scheduler\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "# Training and Evaluation\n",
        "models = {\n",
        "    'LSTM': build_lstm_model(),\n",
        "    '1D CNN': build_cnn_model(),\n",
        "    'Transformer': build_transformer_model()\n",
        "}\n",
        "\n",
        "history = {}\n",
        "aucs = {}\n",
        "race_aucs = {}\n",
        "precision_recall = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    history[model_name] = model.fit(\n",
        "        X_train_resampled, y_train_resampled, validation_data=(X_val, y_val),\n",
        "        epochs=5, batch_size=32, verbose=1,\n",
        "        callbacks=[early_stopping, lr_scheduler]\n",
        "    )\n",
        "\n",
        "    # Compute Overall AUC\n",
        "    y_pred = model.predict(X_test).ravel()\n",
        "    aucs[model_name] = roc_auc_score(y_test, y_pred)\n",
        "    print(f\"Overall AUC for {model_name}: {aucs[model_name]}\")\n",
        "\n",
        "    # Compute AUC per racial group\n",
        "    race_aucs[model_name] = {}\n",
        "    for race in ['asian', 'black', 'white']:\n",
        "        race_idx = np.where(race_test == race)\n",
        "        race_auc = roc_auc_score(y_test[race_idx], y_pred[race_idx])\n",
        "        race_aucs[model_name][race] = race_auc\n",
        "        print(f\"  {race} AUC for {model_name}: {race_auc}\")\n",
        "\n",
        "    # Additional metrics: Precision, Recall, F1-Score\n",
        "    print(f\"Classification Report for {model_name}:\\n\")\n",
        "    print(classification_report(y_test, (y_pred > 0.5).astype(int)))\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "    plt.plot(recall, precision, label=f'{model_name} Precision-Recall')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 6. Visualization: Validation Loss\n",
        "for model_name, hist in history.items():\n",
        "    plt.plot(hist.history['val_loss'], label=f'{model_name} Loss')\n",
        "plt.legend()\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.show()\n",
        "\n",
        "# 7. Summarize Results\n",
        "print(\"Overall AUC Scores:\", aucs)\n",
        "print(\"Race AUC Scores:\", race_aucs)\n",
        "\n"
      ]
    }
  ]
}